\documentclass{llncs}
% \documentclass[twocolumn]{article}

% \usepackage{latexsym}
% \usepackage{amssymb}
\usepackage{graphicx}
% \usepackage{cite}
\usepackage{url}
% \usepackage{soul}
% \usepackage{multirow}
% \usepackage{listings}
% \setstcolor{red}
% \usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}

% \usepackage{comment}
% \usepackage{times}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\hcinv}{\texttt{hc\_inversion}\xspace}
\newcommand{\coqdockw}[1]{\texttt{#1}}
\newcommand{\inversion}{\coqdockw{inversion}\xspace}
\newcommand{\inv}{\coqdockw{inv}\xspace}
\newcommand{\simlight}{\texttt{Simlight}\xspace}
\newcommand{\compcert}{\texttt{CompCert}\xspace}
\newcommand{\clight}{\texttt{Clight}\xspace}
\newcommand{\simsoc}{\texttt{SimSoC}\xspace}
\newcommand{\simsoccert}{\texttt{SimSoC-Cert}\xspace}
\newcommand{\stt}{\small\tt}
\newcommand{\why}{\texttt{Why}\xspace}
\newcommand{\whyML}{\texttt{WhyML}\xspace}
\newcommand{\whyCert}{\texttt{WhyCert}\xspace}
\newcommand{\framac}{\texttt{Frama-C}\xspace}


%replace XXX with the submission number you are given from the ASPLOS submission site.
\newcommand{\asplossubmissionnumber}{28}
\usepackage[normalem]{ulem}
\usepackage{xspace}
\usepackage{alltt}
\usepackage{amsmath}
\usepackage{extarrows}

\pagestyle{plain}
\begin{document}
%date not printed
\date{}

\title{Certified Faithful Simulation}
%\author{Authors omitted for review\\room for authors addresses\\}
\author{%
Vania Joloboff\inst{1}
\and Jean-Fran\c{c}ois Monin\inst{2}
\and Xiaomu Shi\inst{3}
}
\institute{%
East China Normal University - INRIA - LIAMA
\and Universit\'{e} de Grenoble - Verimag
\and Tsinghua University
}
\authorrunning{V. Joloboff, J.-F. Monin \& X. Shi}
\maketitle
\thispagestyle{empty}

\begin{abstract}
  This paper presents an approach to construct a certified virtual
  prototyping framework of embedded software. The machine code
  executed on a simulated target architecture can be proven to provide
  the same results as the real hardware, and the proof is verified
  with an automated theorem prover. The method consists in proving
  each instruction of the instruction set independently, by proving
  that the execution of the C code simulating an instruction yields an
  identical result to that obtained by a formal executable model of
  the processor architecture. This formal model itself is obtained
  through an automated translation process from the architecture
  specifications. Each independent proof draws a number of lemmas from
  a generic lemma library and also uses the automation of inversion
  tactics in the theorem prover. The paper presents the proof of the
  ARM architecture version 6 Instruction Set Simulator of the SimSoC
  open source simulator, with all of the proofs being verified by the
  Coq proof assistant. A significant part of the work has been
  automated by extracting specifications from the reference manual,
  and further automating some Coq tactics to reduce manual work in
  proof development.


\end{abstract}
\section{Introduction}

In many applications nowadays, virtual prototyping is used to design,
develop and test new applications. Most of these virtual prototypes
include an Instruction Set Simulator (ISS) to simulate the target
processor. An ISS can be used for example to optimize algorithms such
as cryptographic software, or to debug new compiler developments, or in
the design of many embedded systems applications.  Instead of using
real hardware prototypes, simulated platforms are more convenient and
less expensive.  Then, it is is important to be sure that the
simulator used is faithful to the hardware that it simulates.

The purpose is to show how it can be certified that the execution of a
binary program on the Instruction Set Simulator for the target
architecture indeed produces the expected results, to be certain that
the data output from the simulator and final processor and memory
state are indeed identical to the result obtained with the real
hardware.  This requires sequential steps, to prove first that the
translation from the C code of the simulator to the simulation machine
is correct, and second that the simulation of the target machine code
is also correct, that is, it preserves the semantics of the
computer architecture, together with the fact that all of these proofs
are verified using a theorem prover or proof checker, not subject to
human error in the proof elaboration or verification.


The next sections of the paper present related work, the existing
tools that we have used, in particular the Coq proof assistant, the
Compcert C compiler, a certified compiler for the C language. The
following section presents our contribution to prove the correctness
of an ARM Instruction Set Simulator, integrated within an existing
virtual prototyping framework. In summary, the method consists in
proving each instruction of the instruction set independently, by
proving that the execution of the C code simulating an instruction
yields identical result to that obtained by a formal executable model
of the architecture. This formal model itself is obtained with
automated translation process from the architecture specifications.
Each independent proof requires using a number of lemmas from a
generic lemmas library and also the automation of inversion tactics in
the theorem prover.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{related}

Program certification has to be based on a formal model of the program
under study.  Such a formal model is itself derived from a formal
semantics of the programming language.
%% JF: standard for FM community
% Formal semantics are important
% because they provide an abstract, mathematical, and standard
% interpretation of a programming language.
% There are three traditional
% ways to express how programs perform computations: axiomatic
% semantics, denotational semantics, and operational semantics.
% An overview of programming language semantics can be found in
% \cite{nielson1992semantics,gunter-1992,winskel-1993}.

%% JF: not really useful
% Denotational semantics \cite{stoy-denotational} constructs
% mathematical objects which describe the meaning of expressions of the
% language using stateless partial \emph{functions}.  All observably
% distinct programs have distinct denotations.

Axiomatic semantics and Hoare logic %\cite{Floyd67,Hoare-1969}
have been widely used for proving the correctness of programs.  For
imperative programming languages such as C, a possible approach is to
consider tools based on axiomatic semantics, like
\framac~\cite{canet2009value}, a framework for a set of interoperable
program analyzers for C. Most of the modules integrated inside rely on
ACSL (ANSI/ISO C Specification Language), a specification language
based on an axiomatic semantics for C.  ACSL is powerful enough to
express axiomatizations directly at the level of the C program.  State
labels can be defined to denote a program control point, and can then
be used in logic functions and predicates.

\framac software leverages off from \why technology
\cite{bobot2011why3,filliatre07cav}, a platform for deductive program
verification, which is an implementation of Dijkstra's calculus of
weakest preconditions.  \why compiles annotated C code into an
intermediate language.  The result is given as input to the VC
(Verification Conditions) generator, which produces formulas to be
sent to both automatic provers or interactive provers like Coq.

%% JF: Not very useful here
% Paolo Herms's \cite{herms13phd}~\cite{herms2012certified} has provided
% a certified verification condition generator for several provers,
% called \whyCert.  This VC generator was implemented and proved sound
% in Coq.  To make it usable with arbitrary theorem provers as
% back-ends, it is generic with respect to a logical context, containing
% arbitrary abstract data types and axiomatisations.

In our case of verifying an instruction set implementation, we have to
deal with a very large specification including complex features of the
C language. A framework is required that is rich enough to make the
specification manageable, using abstraction mechanisms for instance,
and in which an accurate definition of C features is available.
Automated computations of weakest preconditions and range of variation
are not relevant.  As we need to verify specific properties referring
to a formal version of the ARM architecture, operational semantics
offer a more concrete approach to program semantics as it is based on
states. The behavior of a piece of program corresponds to a transition
between abstract states.  This transition relation makes it possible
to define the execution of programs by a mathematical computation
\emph{relation}.  This approach is quite convenient for proving the
correctness of compilers, using operational semantics for the source
and target languages (and, possibly intermediate languages).
%
%% JF: standard, gain room
% Operational semantics can be presented in two styles.
% \textit{Small-step} semantics, often known as structural operational
% semantics, is used to describe how the single steps of computations
% evaluate.  The other is {\em  big-step} semantics, or natural
% semantics, which returns the final results of an execution in one big
% step.
%% JF: standard, gain room
% The corresponding transition relation is defined by rules,
% according to the syntactic constructs of the language, in a style
% which is inspired by natural deduction.
% The book \cite{nielson1992semantics} discusses the alternatives between
% small-step and big-step semantics depending upon the objective.
%
% They sometimes can be equivalent.  But in general, they provide different
% views of the same language and one has to choose an appropriate one
% for a particular usage.  Moreover, some language constructs can be
% hard or even impossible to define with one of these semantics, whereas
% it may be easy with the other style.  In general, when big-step
% semantics can be used, it is simpler to manage than small-step
% semantics. A tutorial on programming language semantics made by
% Benjamin C. Pierce's \cite{pierce-tut} is mainly dedicated to
% operational semantics and the material presented in this tutorial is
% formalized in the Coq proof assistant.
%
Operational semantics are used in \compcert (described below) to
define the execution of C programs, or more precisely programs in the
subset of C considered by the \compcert project.  The work presented
in this paper is based on this approach.  Interesting examples are
given by Brian Campbell in the CerCo
project~\cite{campbell2012executable}, in order to show that the
evaluation order constraints in C are lax and not uniform.

Regarding formalization and proofs related to instruction set, a Java
byte code verifier has been proved by Cornelia Pusch\cite{pusch-1999},
the Power architecture semantics has been formally specified in
\cite{alglave-2009}, and closer to our work, the computer science
laboratory in Cambridge University has used HOL4 to formalize the
instruction set architecture of ARM~\cite{FoxM10}. The objective of
their work was to verify an implementation of the ARM architecture
with \emph{logical gates}, whereas we consider a ARM architecture
simulator coded in C.  Reusing the work done at Cambridge in
\cite{FoxM10} was considered.  But, because we need a certified C
compiler and our approach is based on \compcert C, which is itself
coded in Coq, it would have required us to translate all of the C
operational semantics as well, which would not have been error prone,
not to mention the very large effort. It was more convenient to
develop our formal model and our proofs in Coq.

There is abundant literature covering Instruction Set Simulation.
Using {\em interpretive simulation}, such as used in
Insulin~\cite{insulin-sutarwala-paulin}, each instruction of the
target program is fetched from memory, decoded, and executed.  With
{\em static translation}, the target application program is decoded at
compile time and translated into a new program for the simulation
host. The simulation speed is vastly improved
~\cite{staticISS-zhu-gajski,chung-kyung-staticISS}, but it is not
suitable for application programs that dynamically modify the code, or
dynamically load code at run-time.  Most ISS'es today use some kind of
{\em dynamic binary translation}, initiated with systems such as
Shade~\cite{shade} and Embra~\cite{Embra}, and later enhanced with
various types of optimization techniques~\cite{qemu}.

Our work is based on the SimSoC simulation
framework~\cite{apccas-2008}, which uses dynamic binary translation,
and is available as open source software
at \url{http://gforge.inria.fr/projects/simsoc}.
% JF: cite does notwork here
%~\cite{simsoc-distrib}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background} %
\label{background}

\subsection{Coq}

Coq~\cite{coqart} is an interactive theorem prover, implemented in
OCaml. It allows the expression of mathematical assertions,
mechanically checks proofs of these assertions, helps to discover
formal proofs, and may extract a certified program from the
constructive proof of its formal specification.  Coq can also be
presented as a dependently typed $\lambda$-calculus (or functional
language).  For a detailed presentation, the reader can
consult~\cite{coqmanual} or~\cite{coqart}.  Coq proofs are typed
functions and checking the correctness of a proof boils down to
type-checking.
% For example, a proof term of type $\forall~n: nat,~P\,
% n~\rightarrow Q\, n$ is a function $fun~(n:nat) (p:P\:n)$ which takes
% a natural number $n$ and a proof $p$ of $P\:n$ as arguments and
% returns a proof of $Q\:n$.

Coq is not an automated theorem prover: the logic supported by Coq
includes arithmetic, therefore it is too rich to be decidable.
However, type-checking (in particular, checking the correctness of a
proof) is decidable.  As full automation is not possible for finding
proofs, human interaction is essential.  The latter is realized by
\emph{scripts}, which are sequences of commands for building a proof
step by step.  Coq also provides built-in {\em tactics} implementing
various decision procedures for suitable fragments of the calculus of
inductive constructions and a language
% called \texttt{Ltac} % unused in this paper
which can
be used for automating the search of proofs and shortening scripts.

%% JF: not really useful
% An interactive proof assistant, such as Coq, requires man-machine
% collaboration to develop a formal proof.  Human input is needed to
% create appropriate auxiliary definitions, choose the right inductive
% property and, more generally, to define the architecture of the proof.
% Automation is used for non-creative proof steps and checking the
% correctness of the resulting formal proof.  A rich logic can be
% handled in an interactive proof assistant for a variety of problems.

% On the other hand, fully automated theorem provers have been
% developed.
% % They can perform the proof tasks automatically, that is,
% % without additional human input.  Automated theorem prover can be
% % efficient in some cases.
% However being able to automatically prove a
% formula means that the problems solved are restricted to
% a decidable, or at least semi-decidable, class.
% % Decidable logic are less powerful
% % expressive than higher-order logic, hence the range of
% % problems one can easily or at least conveniently model
% % with an automated theorem prover is smaller
% % than with an interactive proof assistant.
% % In practice, both
% % approaches are important in the fields of computer science and
% % mathematical logic.
% In our project, as a rich logical system is
% needed in order to manage the complexity of the ARM specification and
% of the proofs of C programs, it was decided to use Coq.

\subsection{Compert-C}
\compcert is a formally verified compiler for the C programming
language provided by INRIA~\cite{ccc,Leroy-Compcert-CACM}, which
currently targets Power, ARM and 32-bit x86 architectures.  The
compiler is specified, programmed, and proved in Coq. It aims to be
used for programming embedded systems requiring high reliability
% and is always better than that of gcc without optimizations.
%
% It has a complex translation chain of eleven steps, from C source code
% into assembly code. Every internal representation has its own syntax
% and semantics defined in Coq.
% It is formally verified in the sense that
The generated assembly code is proved to behave exactly the
same as the input C program, according to a formally defined
operational semantics of the language

An important point is that we are considering here C programs
compliant with the definition of ISO-C 99 standard of {\em correct C
  programs}.  Indeed the ISO-C standard identifies many constructions
that are syntactically correct, but have undefined semantics such
as \texttt{a[i++] = i;}.  Compcert-C does not accept such ill-defined
expressions and only well formed programs are translated according to
the formal semantics.
Three parts of \compcert C are used in this work.  The first is that we
use the correct machine code generated by the C compiler.  The second
is the C language operational semantics in Coq
% (its syntax and semantics),
from which we get a formal model of the program.
Third, we use the \compcert Coq library for words,
half-words, bytes etc., and bitwise operations
% and lemmas to describe their properties.
% In our Coq model, we also use these low level
% representations and operations
to describe the instruction set model.

\subsection{SimSoC}
\label{sec:simsoc}
As mentioned above, the certification target ISS is integrated within
\simsoc~\cite{simsoc-2009}, a full system simulator of System-on-Chip
that can simulate various processors, available as open source
software.  \simsoc takes as input real binary code and executes
simulation models of the complete embedded system: processor, memory
units, interconnect, and peripherals.  The chip simulator also
includes a network controller simulator, so that the simulator can
communicate with the real world.

SimSoc uses transaction level modeling (TLM) to
model communications between the simulation modules. It includes
ISS'es to execute embedded applications on
simulated processor. We are considering here the ARM Version 6 ISS.

% Our work results into a new version of the ISS for the ARM architecture.
% in the \simsoc virtual prototyping framework,

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Certified Simulation}
\label{method}

The general objective to
obtain a certified simulator is illustrated in Figure~\ref{fig:diagram}.
Considering the ARM architecture (Version 6), we need to have the following:
\begin{itemize}
\item a simulator of the ARM instruction set in (\compcert) C
  that we can implement from the ARM Reference manual.
\item a formal operational semantics of the ISS. Given some source
  code in C, one can obtain through \compcert the verified machine
  code, or alternatively the Coq formal semantics of the compiled
  program constructed by \compcert. We use both, the compiled machine
  code to run simulations, and the formal semantics for the proof.
\item prove, using a proof assistant, that the resulting ISS semantics
  indeed implement an ARM processor, which boils down to verifying
  that the semantics of the simulator accurately modifies the
  processor (and memory) state representation at each step. For that,
  we need to prove that the results comply with a formal model of the
  ARM architecture.
\end{itemize}

\begin{figure}
% TODO remettre
\hfil\includegraphics[width=0.7\linewidth, trim= 5mm 200mm 15mm 10mm, clip=true]
{fig/diagram.pdf}
\caption{Overall goal}
\label{fig:diagram}
\end{figure}

These steps are described in the following paragraphs.


\subsection{Constructing the formal model}

Ideally the formal specification of the ARM architecture should be
provided by the vendor. But it is not the case, such a formal model is
not available on their web site, hence we had to build one... The only
document available is the ARM reference manual \cite{arm6refman}
(version 6 for this work).  So, it was elected to define the formal model
of ARM architecture in Coq, derived from that architecture reference
manual as output of an automated process. The main relevant chapters
of the manual are:
\begin{itemize}
\item
\texttt{Programmer's Model} introduces the main features in ARMv6 architecture,
the data types, registers, exceptions, etc;
\item
\texttt{The ARM Instruction Set}
explains the instruction encoding in general and puts the instructions in
categories;
\item \texttt{ARM Instructions} lists all the ARM instructions in
  ARMv6 architecture in alphabetical order and the \texttt{ARM
    Addressing modes} section explains the five kinds of
  addressing modes.
\end{itemize}

There are 147 ARM instructions in the ARM V6 architecture.  For each
instruction, the manual provides its encoding table, its syntax, a
piece of pseudo-code explaining its own operation, its exceptions,
usage, and notes.  Except the semi-formal pseudo-code, everything else
is written in natural language.

Three kinds of information are extracted for each ARM operation: its
binary encoding format, the corresponding assembly syntax, and the
instruction semantics, which is an algorithm operating on the various
data structures representing the state of an ARM processor, mostly
registers and memory, according to the purpose of the instruction
considered. This algorithm may call basic functions defined elsewhere
in the manual, for which we provide a Coq library defining their
semantics. Other than these extracted data files, there is still useful
information left in the document which cannot be automatically
extracted, such as validity constraints information required by the
decoder generator.

The formal model in Coq is obtained in three automated steps: (i)
extracting information from the \texttt{.pdf} file completed with some
manual patch to express the relevant constraints; (ii) parsing the
data into abstract syntax trees with a parser generated from grammar;
(iii) automated translation from the abstract syntax into Coq formal
model.

During this process, a dozen documentation problems were found but
none that were relevant to instruction semantics.  The model has then
tested then on real programs to verify that we obtain the same result
as the real hardware. Of course this is not a formal proof, but gives
reasonable confidence in the model.

\subsection{Proof Structure}

The proof starts from an ISS coded in C, where each instruction is
coded as a C function that modifies the processor state and possibly
the memory state (but everything is represented in memory on the
simulation host machine). Each C function may also call basic
functions from a library. This C code is correct C code, accepted by
\compcert C, and does not include any construction with ``unspecified
behavior'' of the C language specification. To prove that the simulator
is correct, we need to prove that, given the initial state of the
system, the execution of an instruction as implemented by a C function
results in the same state as the formal specification. To establish
the proof a formal model of that C implementation is provided by
\compcert, which defines operational semantics of C formalized in Coq.

\begin{figure}[htb]
\hfil\includegraphics[width=.70\linewidth]{fig/theoremca.pdf}
\caption{Theorem statement for a given ARM instruction}
\label{fig:theoca}
\end{figure}

The proof shall demonstrate that the operational semantics of the C
code correspond to the ARM formal specification. The complete proof is
too lengthy for this article, and we only provide here an outline of
the method.  The state of the ARM V6 processor defined in the formal
model is called the \emph{abstract state}.  Alternatively, the same
state is represented by the data structures corresponding to C
semantics that we shall call the \emph{concrete state}.  In order to
establish correctness theorems we need to relate these two models.
Executing the same instruction on the two sides produces a pair of new
processor states which should be related by the same
correspondence. Informally, executing the same instruction on a pair
of equivalent states should produce a new pair of equivalent states,
as schematized by Figure~\ref{fig:theoca}.

The proof consists in defining provable projections from the C state
to the formal model to prove their equivalence, as represented in
Figure~\ref{fig:proj}.
\begin{figure}[h]
\hfil\includegraphics[width=.75\linewidth]{fig/projection.pdf}
\caption{Projection}
\label{fig:proj}
\end{figure}

\subsection{Projection}
As the simulator C program also has the objective to achieve a high
speed simulator, there are optimizations. In particular, processor
state representation in the C implementation is complex, not only due
to the inherent complexity of the C language memory model, but also
because of optimization and design decisions targeting efficiency.

Even though the C representations use a complex memory model, the
\compcert C semantics make it possible to define and prove the
projection function. Fortunately, all of the instructions
operate on the processor state and there is a single representation of
that state in the simulator. It is necessary and sufficient to prove
the projection for each particular case of the representation
structure. For example, the projection of a register performs a case
analysis on possible values, whereas the projection of saved data upon
exceptions depends on the type of exception modes.  Although there are
a number of specific cases to handle, the proof of the projection is
relatively straightforward.  In more detail:
\begin{itemize}
\item The C implementation uses large embedded \emph{struct}s to
  express the ARM processor state.  Consequently the model of the
  state is a complex Coq record type, including not only data fields
  but also proofs to verify access permission, next block pointer,
  etc.
\item Transitions are defined with a relational style (as opposed to a
  functional style where reasoning steps can be replaced by
  computations). As the kind of record type mentioned in the previous
  item is too complex to execute computations with it, it is
  convenient to describe the state transformations for memory with a
  relation, which fits well with operational semantics.
\item The global state is based on a memory model with load
  and store functions that are used for read/write operations.
\end{itemize}

The proofs for instructions start from the abstract state described by
the formal specification.  In order to verify the projection of the
original state, we need the following data: the initial
memory state, the local environment, and the formal initial processor
state.  The projection is meaningful only after the C memory state is
prepared for evaluating the current function body representing a ARM
instruction.  In the abstract Coq model, we directly use the processor
state \texttt{st}.  But on the C side, the memory state must provide
the contents of every parameter, especially the memory representation
of the processor state.  We also need to observe the modifications of
certain blocks of memory corresponding to local variables.

% Fortunately \compcert formalizes the C memory model.
The semantics of \compcert C consider two environments. The global
environment \emph{genv} maps global function identifiers, global
variables identifiers to their blocks in memory, and function pointers
to a function definition body.  The local environment \emph{env} maps
local variables of a function to their memory blocks reference.  It
maps each variable identifier to its location and its type, and its
value is stored in the associated memory block.  The value associated
to a C variable or a parameter of a C function is obtained by applying
\texttt{load} to the suitable reference block in memory.  These two
operations are performed when a function is called, building a local
environment and an initialized memory state. When the program starts
its execution, \emph{genv} is built.  The local environment \emph{env}
is built when the associated function starts to allocate its
variables. Therefore, on the concrete side, a memory state and a local
environment is prepared initially using two steps. First, from an
empty local environment, all function parameters and local variables
are allocated, resulting into some memory state and the local
environment. Second, function parameters are set up using a dedicated
function \texttt{bind\_parameters} and the initial state is thus
created.

\subsection{Lemmas Library}

Next, we need to consider the execution of the instruction.
In the C ISS, there is a standalone C function
for each ARM V6 instruction.  Each function (instruction) has its own
correctness proof.  Each function is composed of its return
type, arguments variables, local variables, and the function body. The
function body is a sequence of statements including assignments and
expressions. Let us consider as an example the ARM instruction
\texttt{BL} (\texttt{Branch and Link}). The C code is:
\small{
\begin{verbatim}
void B(struct SLv6_Processor *proc,
       const bool L,
       const SLv6_Condition cond,
       const uint32_t signed_immed_24){
 if (ConditionPassed(&proc->cpsr, cond)){
  if ((L == 1))
   set_reg(proc,14,address_of_next_instruction(proc));
   set_pc_raw(proc,reg(proc,15)+(SignExtend_30(signed_immed_24)<<2));
 }
}
\end{verbatim}
}

\compcert has designed semantics for \compcert C in big-step inductive
types for evaluating expressions, which we reuse for the proof.  The
semantics is defined as a relation between an initial expression and
an output expression after evaluation.  Then, the body of the function
is executed.  On the concrete side, the execution yields a new state
\textbf{mfin}.  On the abstract side, the new state is
obtained by running the formal model.
% The formal model of ARM V6 is defined as a much simpler functional
% model and computing the value of a component can be performed
% directly.
We have to verify that the projection from the
concrete state \textbf{mfin} is related to this abstract
state.
% Note that all projections are
The proof is performed in a top-down manner. It follows the definition
of the instruction, analyzing the expression step by step.  The
function body is split into statements and then into expressions.
When evaluating an expression, one has to search for two kinds of
information.  the first one is how the memory state changes on the
concrete side; the other is whether the results on the abstract and
the concrete model are related by the projection.  To this end, a
library of lemmas had to be developed, identifying five categories
summarized below:

\begin{enumerate}
\item
  \textit{Evaluating a \compcert expression with no modification on the memory state.}\\
  Such lemmas are concerned with the expression evaluation on \compcert
  C side and in particular the C memory state change issue.  Asserting
  that a memory state is not modified has two aspects: one is that the
  memory contents are not modified; the other is that the memory
  access permission is not changed.  For example, evaluating the
  boolean expression $Sbit~==~1$ returns an unchanged memory state.
\[
\begin{array}{l}
\textrm{if}~~ G,E~\vdash \texttt{eval\_binop}_c~(Sbit~==~1),
\:M~\xLongrightarrow{\varepsilon}~v,\:M'
\\
\textrm{then}~~ M=M'.
\end{array}
\]
In Coq syntax, the relation in premise is expressed with
\texttt{eval\_binop}.
%a companion predicate of \texttt{exec\_stmt} above, devoted to binary operations.
In this lemma and the following,
$E$ is the local environment, $G$ is the global environment, $M$ is
the memory state, $\varepsilon$ is the empty event
% (\texttt{Events.E0} in Coq syntax);
(we may have here a series of events) and $v$ is the result.
% Here, $vres$ is not important.
The evaluation is performed under environments $G$ and $E$.  Before
evaluation, we are in memory state $M$.  With no event occurring, we
get the next memory state $M'$. According to the definition of
\texttt{eval\_binop}, an internal memory state will be introduced.
\begin{center}
$\dfrac
{G,E~\vdash a_1,M\Rightarrow M'~~~G,E~\vdash a_2,M'\Rightarrow M''
}
{G,E~\vdash (a_1~binop~a_2),M\Rightarrow~M''}$
\end{center}

In the example, expression $a_1$ is the value of $Sbit$ and $a_2$
is the constant value $1$.  By inverting the hypothesis of type
\texttt{eval\_binop}, we obtain several new hypotheses, including on
the evaluation of the two subexpressions and the introduction of an
intermediate memory state $M''$.  Evaluating them has no change on the
C memory state, hence we have $M = M'' = M'$.
In more detail, from the \compcert C semantics definition, we know that
the evaluation of an expression will change the memory state
if the evaluation contains uses of \texttt{store\_value\_of\_type}
in \compcert versions before 1.11.
% which stores the value in memory at a given block reference and
% memory chunk.
In \compcert-1.11, the basic store function on memory
is represented by an inductive type \texttt{assign\_loc} instead of
\texttt{store\_value\_of\_type}.
% Since \compcert version 1.11 introduced volatile memory access,
% we have to determine whether the object type is volatile before storage,
% and also type size in addition of the access mode.

\item
\textit{Result of the evaluation of an expression with no modification on the memory.}\\
Continuing the example above, we now discuss the result of evaluating
the binary operation $Sbit~==~1$ both in the abstract and the concrete model.
At the end of evaluation, a boolean value $true$ or $false$ is returned
in both the concrete and the abstract models.
\[
\begin{array}{l}
\textrm{if} ~ \texttt{Sbit\_related}~M~\texttt{Sbit},\\
\textrm{and} ~ G,E~\vdash \texttt{eval\_rvalue\_binop}_c~(Sbit~==~1),  M\Rightarrow~v,M'\\
\textrm{then} ~ v=(Sbit~==~1)_{coq}
\end{array}
\]
Intuitively, the projection corresponding to the parameter
\texttt{sbit} in the concrete model must yield the same value as in
the abstract model.  Here, the expression is a so-called ``simple
expression'' that always terminates in a deterministic way, and
preserves the memory state.  To evaluate the value of simple
expressions, \compcert provides two big-step relations
\texttt{eval\_simple\_rvalue} and \texttt{eval\_simple\_lvalue} for
evaluating respectively their left and right values.  The rules have
the following shape:
\[
\dfrac{
\begin{array}{l}
G,E~\vdash a_1,M\Rightarrow v_1 \quad G,E~\vdash a_2,M\Rightarrow v_2\\
\texttt{sem\_binary\_operation}(op,v_1,v_2,M)~=~v
\end{array}}
{G,E~\vdash (a_1~op~a_2),M\Rightarrow v}
\]
In order to evaluate the binary expression $a_1~op~a_2$,
the sub-expressions $a_1$ and $a_2$ are first evaluated,
and their respective results $v_1$ and $v_2$ are used
to compute the final result $v$.

\item
  \textit{Memory state changed by storage operation or side effects of evaluating expression.}\\
  As mentioned before, evaluating some expressions such as
  \texttt{eval\_assign} may modify the memory state.  Lemmas are
  required to state that corresponding variables in the abstract and
  in the concrete model must evolve consistently.  For example,
  considering an assignment on register $Rn$, the projection relation
  \texttt{register\_related} is used. Expressions with side effects of
  modifying memory are very similar.
\[
\begin{array}{l}
\textrm{if} ~~ \texttt{rn\_related}~M~rn\\
\textrm{and}~~  G,E~\vdash \texttt{eval\_assign}_c~(rn:=rx),M~\Rightarrow~ M',v\\
\textrm{then} ~~ \texttt{rn\_related}~M'~rn
\end{array}
\]

\item
  \textit{Internal function call.}\\
  Internal functions are described in an informal manner in the ARM V6
  reference manual.  No pseudo-code is available for them, which means
  that the corresponding library functions, both in the abstract model
  and in C, are coded manually. When combining the simulation code of
  an instruction with the code of library functions, one needs to
  verify potential memory allocation issues. After an internal
  function is called, a new stack of blocks is typically allocated in
  memory.  After the evaluation of the function, these
  blocks will be freed.  Unfortunately, this may not bring the memory
  back to the previous state: the memory contents may stay the same,
  but the pointers and memory organization may have changed.
\label{page:libfunast}
\[
\begin{array}{l}
\textrm{if} ~~  \texttt{proc\_state\_related}~M~st \\
\textrm{and} ~~ G,E~\vdash \texttt{eval\_funcall}_c (copy\_StatusRegister)_c,M\Rightarrow~v,~M'\\
\textrm{and} ~~ st'~=~(copy\_StatusRegister)_{coq}~st\\
\textrm{then} ~~\texttt{proc\_state\_related}~M'~st'.
\end{array}
\]

Lemmas must be developed regarding the evaluation of internal
functions, so that one can observe the returned results,
compare it with the corresponding evaluation in the formal
specification, and verify some conditions.  For example, the lemma
above is about the processor state after evaluating an internal
function call \texttt{copy\_StatusRegister}, which reads the value of
the CPSR (Current Processor Status Register) and copies it into
the SPSR (Saved Processor Status Register) when an exception occurs.  The
evaluation of \texttt{copy\_StatusRegister} must be protected by a
check on the current processor mode.  If it is in authorized mode, the
function \texttt{copy\_StatusRegister} can be called.  Otherwise, the
result is ``unpredictable'', which is defined by ARM architecture

It is necessary to reason on the newly returned states, which
should still be related by the projection.  This step is usually easy
to prove, by calculation on the two representations of the processor
state to verify that they match.

\item
\textit{External function call.}\\
The \compcert C AST of an external function call contains the types of
input arguments and of the returned value, and an empty body.
\compcert provides the expected properties of a few built-in external
functions such as \texttt{printf}, \texttt{malloc} and \texttt{free}.
A similar process is used for the external functions of the ARM
simulator.  The general expected properties of an external call are
that (i) the call returns a result, which has to be related to the
abstract state, (ii) the arguments must comply with the
signature.  (iii) after the call, no memory blocks are invalidated,
(iv) the call does not increase the access permission of any valid
block, and finally that the memory state can be modified only when the
access permission of the call is granted. For each external call,
such required properties are verified.
\end{enumerate}

In addition to the above lemmas we had to prove a fair number of more
trivial lemmas that are omitted here.  Most of them are related to the
semantics of \compcert C. They are all gathered into a library of
lemmas used to construct the individual instructions proofs.

\subsection{Inversion And Instruction Proofs}
% Details are given in~\cite{xiaomu-phd}.
Equipped with these lemmas we can build the proof scripts for ARM
instructions.  For that, we are decomposing the ARM instruction
execution step by step to perform the execution of the C programs
. \compcert C operational semantics define large and complex inductive
relations. Each constructor describes the memory state transformation
of an expression, statement, or function.  As soon as we want to
discover the relation between memory states before and after
evaluating the C code, we have to invert the hypotheses of operational
semantics to follow the clue given by its definition, to verify the
hypotheses relating concrete memory states according to the
operational semantics.

During the development of a proof, if a hypothesis is an instance of
an inductive predicate and we want to derive the consequences of this
hypothesis, the general logical principle to be used is called
\emph{inversion}. An {\em inversion} is a kind of forward reasoning
step that allows for users to extract all useful information contained
in a hypothesis.  It is an analysis over the given hypothesis according
to its specific arguments, that removes absurd cases, introduces
relevant premises in the environment and performs suitable
substitutions in the whole goal.  The practical need for automating
inversion has been identified many years ago and most proof assistants
(Isabelle, Coq, Matita,...)  provide an inversion mechanism.  To this
effect, the Coq proof assistant provides a useful tactic called
\inversion \cite{coqmanual}.

Every instruction contains complex expressions, but each use of
\inversion will go one step only.  If we want to find the relation
between the memory states affected by these expressions, we have to
invert many times. For illustration, let us consider the simple
example from the ARM reference manual \texttt{CPSR = SPSR}.  As
the status register is not implemented by a single value, but a set of
individual fields, the corresponding C code is a call to the function
\texttt{copy\_StatusRegister}, which sets the CPSR field by field with
the values from SPSR.  Lemma \texttt{same\_cp\_SR} below states that
the C memory state of the simulator and the corresponding formal
representation of ARM processor state evolve consistently during this
assignment.
\begin{alltt}\small
Lemma same_copy_SR :
  \(\forall\) e m l b s t m' v em,
  proc_state_related m e (Ok tt (mk_semstate l b s)) \(\rightarrow\)
    eval_expression (Genv.globalenv prog_adc) e m expr_cp_SR t m' v  \(\rightarrow\)
    \(\forall\) l b, proc_state_related m' e
                      (Ok tt (mk_semstate l b (Arm6_State.set_cpsr s
                                              (Arm6_State.spsr s em))))
\end{alltt}
From this, we have to invert generated hypotheses until all
constructors used in its type are exhausted. On this example, 18
consecutive inversions are needed... The first proofs scripts we wrote
were becoming unmanageable, and not robust to version changes of Coq
or \compcert.  In order to reduce the script size and get better
maintainability, we studied a general solution to this problem, and
developed a new inversion tactics in Coq
%\cite{small-inversion,itp13}
The standard inversion mechanism from Coq has been expanded into a new
inversion tactic for inductive types in \compcert.  The semantics of
\compcert C tells how the memory state is transformed by evaluating
expressions.  Using the built-in constructs of the tactics language,
one can define a high-level tactic for each inductive type, gathering
all the functions defined for its constructors.

This tactic has two arguments corresponding to the C memory states.
The first step of the tactic introduces generated components with new
names.  The second step is related to previously reverted hypotheses,
ensuring that the new names introduced are correctly managed by Coq.
The tactic then proceeds as follows. First, it automatically finds the
hypothesis to invert by matching the targeted memory states. Then the
related hypotheses are reverted and an appropriate auxiliary function is
called (all auxiliary functions are gathered into the tactic) and
meaningful names are given to derived variables and hypotheses. Next,
all other related hypotheses are updated according to the new names,
and finally new values and useless variables or hypotheses are cleaned up.
The above steps are then repeated until all transitions between the
two targeted memory states are discovered.
\noindent
As a result, considering the former example of \texttt{same\_copy\_SR}
where 18 standard \inv were used in the first proof script we
developed, the new tactics reduced them into one single step:
\texttt{inv\_eval\_expr~m~m'}.

Thanks to the base lemmas and this new tactics, the size of the proofs
has become much smaller and the proof scripts are more manageable. The size
vary with the instructions complexity from less than 200 lines (e.g
170 for LDRB) to over 1000 (1204 for ADC).  As a result, for each ARM
instruction, we have established a theorem proving that the C code
simulating an ARM instruction is equivalent to the formal
specification of the ARM processor.

% \section{Certified Simulation}

% Based on the work afore mentioned, we can now consider the certified
% execution of a C program. We take here as an example the DES
% cryptographic encryption code.  The C code for encrypting a block of
% data is straightforward:
%
% {\small\begin{verbatim}
% #define GET_ULONG_BE(n,b,i)
%  (n)=((unsigned long)(b)[(i)]<<24)
%     |((unsigned long)(b)[(i)+1]<<16)
%     |((unsigned long)(b)[(i)+2]<<8)
%     |((unsigned long)(b)[(i)+3] );

% #define DES_IP(X,Y)
%     T =((X>>4)^Y)&0x0F0F0F0F;
%     Y^=T;X^=(T<<4);
%     T=((X>>16)^Y)&0x0000FFFF;
%     Y^=T;X^=(T<<16);
%     T=((Y>>2)^X)&0x33333333;
%     X^=T;Y^=(T<<2);
%     T=((Y>>8)^X)&0x00FF00FF;
%     X^=T;Y^=(T<<8);
%     Y=((Y<<1)|(Y>>31))&0xFFFFFFFF;
%     T=(X^Y)&0xAAAAAAAA;
%     Y ^=T;X^= T;
%    X=((X<<1)|(X>>31))&0xFFFFFFFF;

% #define DES_ROUND(X,Y)
%     T=*key++^X;
%     Y^=SB8[(T)&0x3F]
%        ^SB6[(T>>8)&0x3F]
%        ^SB4[(T>>16)&0x3F]
%        ^SB2[(T>>24)&0x3F];
%     T=*key++^((X<<28)|(X>>4));
%     Y^=SB7[(T)&0x3F]
%        ^SB5[(T>>8)&0x3F]
%        ^SB3[(T>>16)&0x3F]
%        ^SB1[(T>>24)&0x3F];

% void des_crypt_ecb(unsigned long *key,
%                unsigned char input[8],
%              unsigned char output[8] ){
%     int i;
%     unsigned long X,Y,T;
%     GET_ULONG_BE(X,input,0);
%     GET_ULONG_BE(Y,input,4);
%     DES_IP(X,Y);
%     for(i=0;i<8;i++) {
%         DES_ROUND(Y,X);
%         DES_ROUND(X,Y);
%     }
%     DES_FP(Y,X);
%     PUT_ULONG_BE(Y,output,0);
%     PUT_ULONG_BE(X,output,4);
% }
% \end{verbatim}
% }

% Looking at the binary code of that function generated by the compiler,
% one may observe that this code actually uses only 22 different types of ARM
% instructions, namely \textbf{ add, and, asr, b, ble, bne, bx, cmp,
%   eor, ldm, ldr, ldrb, lsl, lsr, mov, orr, pop, push, str, str, strb,
%   sub}.

% Given that we have a proof that the machine code generated from C is
% correct, thanks to \compcert, and now a proof of the ARM instruction
% set for these instructions, we have a proof that the simulation of the
% DES algorithm on our ARM simulator is conformant with the algorithm.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{conclusion}

Using the approach presented in this paper, we can construct a tool
chain that makes it possible to certify that the simulation of a
binary executable program on some simulation platform is compliant
with the formal model of the target hardware architecture.

Using Compcert-C, that has defined formal C semantics, we have
formally proved, using the Coq theorem prover to automate the proof,
the ARM v6 Instruction Set Simulator of SimSoc.  There is no limit on
the size of the code that can be simulated as long as the target
instruction set has been certified.

We certainly acknowledge the limits of our approach: the quality of
our ``certified simulation'' relies on the faithfulness of our formal
model of the ARM processor to the real hardware. Because the vendor
companies do not provide a formal description of their hardware, one
has to build them\footnote{Note that this problem is the same as for
  the work done by Cambridge University}. The formal model used in
this work was built mostly in automated manner from the vendor
reference manual by extracting the pseudo-code from the manual and
translating this code into a Coq formal model. Another potential issue
is that ARM is licensing the technology to electronics companies, and
it is not certain either that the chips manufactured by these
licensees are truly compliant with the original specification that
they license, but this is well beyond the scope of this paper.  If the
vendors would make public formal specifications of their
architectures, then our toolchain would become fully verified.

We believe this work has further impact on proofs of programs.  First,
we have proved here a significantly large C program.  Second, because
the proved program is a hardware simulator, it can be used as a tool
to prove execution of target programs.  For example considering a
cryptographic algorithm immplemented for the ARM archiecture and
compiled with Compcert-C, it could then be proved that the execution
of that program provides the exact encryption required, and nothing
else.  Therefore, the tool presented is an enabler for the proofs of
other programs, which offers a direction for future research.

Another consequence of this work is that, supposing one could
compile the C instructions to silicon using a silicon compiler,
and that compiler would also be certified, ala \compcert C,
it would then make it possible to prove real hardware.

% As a side note, the generator from the documentation also generates
% the decoder of binary instructions supported in ARM
% architectures. This decoder is obtained by compiling the opcodes
% information, so we are confident that the decoder from binary to call
% the appropriate C function is correct, and has been tested on
% hundreds of millions of cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Acknowledgments}

% Removed for review.
% This work has been supported jointly by INRIA, Tsinghua University
% and CNRS, Vania Joloboff is currently visiting Professor at East
% China Normal University.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\bibliographystyle{IEEEtran}
\bibliographystyle{abbrv}
\bibliography{references}


\end{document}
